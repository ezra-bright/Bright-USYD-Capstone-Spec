  
# Scraping in practice

I hope at the very least you know what a scraper is! A scraper extracts particular pieces of data from the web, usually assembling them into a database of some sort. I recommend you get familliar with
- BeautifulSoup
- XPaths & Selectors in the DOM

(Got no idea what they are? Ask chatgpt :) )

The process of scraping can be a very laborious and error-prone process. It is important that you come to and understanding of how it is done so you know what can be made better, and what can be automated.

**Exercise**:

a) Go to [https://www.theguardian.com/media/all](https://www.theguardian.com/media/all), and try to think about how you could describe to a computer 
where to find the links to articles. Inspect element and observe the structure of the components. 

b) It is easy to describe the location of where to click in order to generate more articles using it's XPath (Inspect element -> right-click element -> copy -> copy XPath). But now think about how a smart scraper might figure out where this location is.

# Goal

The goal of the smart scraper is to create an engine that can **greatly reduce the effort** involved in automatically obtaining **structured data** from the web. The ideal case is where the engine can take an abstract description of the information to be obtained, and then generate a computer-readable output that describes how to scrape the webpage. 


# Your Task


## 1. Understand the problem and ideate approaches
Dont be a data-scientist with all of the data but none of the science! Try making (or at the very least thinking about) your own web scraper to scrape a website such as the guardian. Try to consider other applications, eg the usyd/unsw/uts handbook websites, and observe the kinds of challenges you may face.

## 2. Agree on an input and output specification
The example specification given is only indicative, and is undoubtably going to evolve as you think about the problem, and explore your capabilities. You must design and input and output specification such that:
- The conversion from input to output is achievable with an explicit method.
- The method's validity is decently general. This is a hard problem is not expected that the engine works every time - I would be impressed if it gave mostly accurate results for a fraction of all websites.
- The input is suitable to be generated by a human. You need to be able to justify to us that your input specification is substantially easier to use than manual scraping methods
- It is possible for a general scraper to operate based on the output data. You need to describe (but not necessarily implement) a method by which the output format may be parsed to perform the scraping operation.


## 3. Develop the engine 
Considering that you do not have a scraper to run the generated results, the enging you create will primarily be assessed on:

1. How optimal the state transitions are. Optimality would encompass minimizing the number of nodes, and minimizing the number of websites that would be required.
2. Whether the correct data for the input schema can be found wherever it is claimed to be found.

For testing purposes, it may be helpful to write small parts of the general scraper. For example, you could write a program which takes the output for a particular state, and then locates all the text on the page.

## 4. Develop the general scraper
If you have time to develop the scraper which takes the engine's output as input, that would be spectacular! 

___


# Sample Specification


## Input

The input consists of an entrypoint webpage, and a set of objects with fields:

Entrypoint URL: [https://www.theguardian.com/au](https://www.theguardian.com/au)
```json
{
	NewsArticle: {
		title: 'the news article title',
		authors: 'the names of the authors of the article',
		date: 'The date on which the article was published',
		paragraphs: 'A list of paragraphs in this news article'
	},
	Writer: {
		name: 'full name of the writer',
		articles: 'list of articles that this author wrote'
	}
}
```

___


## Output

Note that in practice, the paths list is going to be much longer. I am but a human, and do not have time to scrape every link. You can think of each path string as a specific format and location that the navigation link could be in. Lots of webpages can have different formats depending on their type - the guardian is a good example of this. Note the `*` in the path denoting that this path matches any number.

[https://www.theguardian.com/au](https://www.theguardian.com/au)

```json
{
	root: { // [https://www.theguardian.com/au](https://www.theguardian.com/au)
		LINKS: {
			state1: { // Article page
				paths: [ // Each of these is a pattern for an article link to match
					"//*[@id="container-36b9cf13-bf8b-4cf0-b882-5c5d4feac0a2"]/div[1]/ul/li[*]/div/div/a",
					"//*[@id="container-36b9cf13-bf8b-4cf0-b882-5c5d4feac0a2"]/div[2]/ul/li[*]/ul/li[*]/div/div/a",
					"//*[@id="container-bbeca920-4543-4db5-b315-8a39a5e0642f"]/div/ul/li[*]/ul/li[*]/div/div/a"
				],
				action: 'click', // Action to perform at this path
                // We might get false positives, so the criterion confirms whether what we have is correct
                criterion: <check that navigation occured and new url matches [https://www.nytimes.com/YYYY/MM/DD/](https://www.nytimes.com/YYYY/MM/DD/)>
			},
			root: { // Scrolling stays on the same page
				paths: [ 
					"//*[@id="container-36b9cf13-bf8b-4cf0-b882-5c5d4feac0a2"]"
				],
				action: 'scroll_to_bottom',
				criterion: <check that more articles loaded>
			},
		},
		FIELDS: {}
	},
	state1: { // Article page
		LINKS: {
			state3: {
				paths: ["/html/body/main/article/div/div/div/aside[2]/div/div/div/div[1]/div/address/div/a[*]"],
				action: 'Click', // Action to perform at this path
				criterion: <check that navigation occured and new url matches [https://www.nytimes.com/YYYY/MM/DD/](https://www.nytimes.com/YYYY/MM/DD/)>
			}
		}, 
		FIELDS: {
			NewsArticle: {
				title: ["/html/body/main/article/div/div/div/div[3]/div/div/h1"],
				authors: ["/html/body/main/article/div/div/div/div[3]/div/div/h1"],
				date: ["/html/body/main/article/div/div/div/aside[2]/div/div/div/div[1]/div/details/summary/span"],
				paragraphs: ["//*[@id="maincontent"]/div/p[*]"]
			},
		}
	},
	state3: { // Author page
		LINKS: {},
		FIELDS: {
			Writer: {
				name: ["//*[@id="top"]/div[3]/div/div[1]/div/h1"],
				articles: ["//*[@id="container-March 2023"]/div/ul/li[*]/div/div/a"]
			}
		}
	},
}


```


The attatched diagram illustrates the state transitions, and how they relate to the actions and urls that are navigated to.


If you would like a less technical way of communicating how the scrapers work, watch the week 3 meeting recording. Heere is some python-esque pseudocode that describes the operation of a dumb scraper. Note the addition of the action type and criterion to my explanation from the meeting.

```Python
DUMB_SCRAPER_INPUT = loadJsonFromFile('smart_scraper_output.json')
ROOT_PAGE_URL = 'https://www.theguardian.com/au'

# Keep track of all the urls that we have finished scraping.
all_finished_urls = [] 


# Empty dictionary
scraped_objects = {
	'NewsArticle': [],
	'Writer': []
}


# This is recursive because its easier to write - this is basically a depth-first search. In practice its a very idiotic implementation (wont someone think of the memory usage!?!), but I hope it conveys what needs to be done.

function scrapePage(curr_page, curr_state):	

	# On this page, extract all the fields from the paths 
	for each key, value in DUMB_SCRAPER_INPUT[curr_state]['LINKS']:
		object_name = key # Eg NewsArticle
		object_fields = value # Eg {title: [...], authors: [...]}
		extracted_data_object = extract_text_from_paths(curr_page, object_fields)
		scraped_objects[key].append(extracted_data_object)

	# On this page, find all the new pages that can be navigated to from this one
	for each key, value in DUMB_SCRAPER_INPUT[curr_state]['LINKS']:	
		next_state = key # eg 'state1' 
		state_info = value # eg SMART_SCRAPER_OUTPUT[state]['LINKS']['state1']

		# This is the data representation for the criteron in order to tell whether a link is what we are looking for.
	    criterion = state_info['criteron']
	
	    # Go to all paths 
		for each path_pattern in state_info['paths']:
			# Get a list of all html elements which satisfy this path
			all_potential_link_element_list = current_page.find_all_pattern_matches(path_pattern)
			for each potential_link in all_potential_link_elements_list:
				 # Assume the current_page variable stays the same, but the result of the action creates a new page with that action performed. The new page could have the same url oas the old page.
				action = state_info['action'] # eg 'click'
				criterion = start_info['criterion'] # Eg 1: curr_page.url != new_page.url. Eg 2: curr_page.html.length < new_page.html.length
				next_page = performAction(curr_page, action)
				# Test the page to check that we havent already finished scraping it, and test the page to see if it satisfies the criterion.
				if next_page.url not in all_finished_urls and criterion(next_page, curr_page):
					# Now scrape the new page, and declare you have
					# transitioned to the next state
					scrapeUrl(next_page, next_state)

	# After all that, we are finished with this page.
	# add it to finished URL so that we make sure it doesnt get scraped again.
	all_finished_urls.append(curr_page.url)
				
			

# Program entrypoint. Beginning state is root.
scrapeUrl(page=openPage(ROOT_PAGE_URL), curr_state='root')

```


# Things to think about:
- How will different types of data (eg lists, numerical, etc) be communicated in the input specification? Will the input spec need more fields to achieve this?
- What kinds of page validation criterion would you have? How would it be inferred by the scraper
- When there are multiple states with the same information (eg the title of a news artcle can be listed on the main page, but it will also be found on the article itself) How does the engine choose which page to navigate to in order to get this information
- When the engine comes accross a piece of information, how does it know which object the information belongs to?
